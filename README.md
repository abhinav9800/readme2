# readme2

| Feature/Aspect         | Self-Distillation Approach   (Selected)                                                           | ResNet+SE Approach                                                                                     | ResMax Approach                                                                   | Technical Differences/Notes                                                                                                                                     |
|------------------------|---------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Core Concept           | Uses deepest network layer to teach   shallow network layers                                      | Uses multiple feature scales through   hierarchical residual-like connections                          | Combines skip connection (ResNet)   with max feature map (LCNN)                   | Self-distillation focuses on   knowledge transfer within a single model; Res2Net creates multiple feature   scales; ResMax optimizes for lightweight deployment |
| Network   Architecture | Standard CNN backbone (ECANet or   SENet) divided into 4 blocks with classifiers after each block | Res2Net splits feature maps by   channel dimension with hierarchical connections between filter groups | Uses ResMax blocks with MFM layers   after convolution layers                     | Self-distillation doesn't modify the   base architecture; Res2Net and ResMax both modify the building blocks   themselves                                       |
| Training   Methodology | Multi-task learning with deep layer   supervising shallow layers                                  | Standard end-to-end training with   additional feature scale connections                               | End-to-end training with   cost-sensitive learning for imbalanced data            | Self-distillation uses   teacher-student paradigm within one model; others use modified architectures   without internal teaching                               |
| Loss Function          | Hard loss (A-Softmax) + Soft loss (KL   divergence) + Feature loss (L2)                           | Standard cross-entropy loss                                                                            | Binary cross-entropy with   cost-sensitive weighting                              | Self-distillation uniquely uses three   complementary loss components; others use single loss functions with   modifications                                    |
| Feature   Processing   | Converts feature formats between   layers using projection if dimensions don't match              | Creates multiple receptive fields   within one block                                                   | Uses MFM to select maximum   activations between feature maps                     | Self-distillation transfers knowledge   across layers; Res2Net enlarges receptive fields; ResMax performs explicit   feature selection                          |
| Inference   Complexity | No additional cost (auxiliary   classifiers removed after training)                               | Slightly more complex than ResNet due   to additional connections                                      | Reduced complexity through MFM   operations                                       | Self-distillation has zero inference   overhead; ResMax specifically designed to be lightweight                                                                 |
| Parameter   Efficiency | Reuses parameters across multiple   classifiers during training                                   | Reduces parameters by splitting   filters and using hierarchical connections                           | Uses MFM to halve number of feature   maps                                        | Self-distillation focuses on training   efficiency; Res2Net and ResMax focus on structural efficiency                                                           |
| Best   Performance     | EER: 0.88% (LA), 0.74% (PA)                                                                       | EER: 0.74% (PA), 2.87% (LA)                                                                            | EER: 0.37% (PA), 2.19% (LA)                                                       | ResMax performs best on PA dataset;   Self-distillation and Res2Net models perform best on LA dataset in their   respective papers                              |
| Optimal   Features     | F0 subband (low-frequency features)                                                               | CQT performs best for both LA and PA                                                                   | CQT with specific parameters   (fmin=1Hz, K=120)                                  | All approaches find spectral features   crucial; CQT appears consistently effective across approaches                                                           |
| Generalizability       | Improves performance for both known   and unknown attacks                                         | Explicitly designed for   generalization to unseen attacks                                             | Tests with different feature   extraction parameters to optimize generalizability | Self-distillation improves shallow   networks; Res2Net adds multiple feature scales; ResMax optimizes lightweight   architecture                                |
